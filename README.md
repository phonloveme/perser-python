#LAB1\TASK1
1. Как работает программа в целом?
Программа выполняет следующие шаги:

Получение HTML-страницы : Используется библиотека requests для отправки HTTP-запроса к странице и получения её содержимого.
Парсинг HTML : С помощью библиотеки BeautifulSoup извлекаются все теги <a> (гиперссылки), которые указывают на PDF-документы.
Обработка ссылок : Относительные ссылки преобразуются в абсолютные с помощью метода urljoin, а также извлекается текст ссылки (название документа).
Запись данных в CSV : Полученные данные сохраняются в файл формата CSV с разделителем ;.

2. Как работает метод requests.get(url)?
Метод requests.get(url) отправляет HTTP-запрос типа GET на указанный URL (https://pcoding.ru/darkNet.php) и возвращает объект Response.
Этот объект содержит:
Код состояния HTTP (например, 200 — успех, 404 — страница не найдена).
Тело ответа (HTML-код страницы).
Мы используем response.text для получения HTML-кода страницы.
Важно : Если страница использует кодировку, отличную от UTF-8, мы явно указываем её через response.encoding = 'utf-8'.

3. Что делает BeautifulSoup(response.text, 'html.parser')?
BeautifulSoup — это мощная библиотека для парсинга HTML и XML.
Конструктор BeautifulSoup принимает два аргумента:
HTML-код страницы (response.text).
Парсер ('html.parser' — встроенный парсер Python).
После создания объекта soup можно использовать его методы для поиска элементов на странице.

5. Как работает метод soup.find_all('a', href=True)?
Метод find_all ищет все теги, соответствующие заданным критериям.
В данном случае:
'a' — ищем теги <a> (гиперссылки).
href=True — фильтруем только те теги <a>, у которых есть атрибут href (т.е. ссылки).
Результат — список всех подходящих тегов.

7. Как проверяется, что ссылка ведёт на PDF?
Условие if href.lower().endswith('.pdf'): проверяет, заканчивается ли значение атрибута href на .pdf (без учёта регистра).
Метод lower() приводит строку к нижнему регистру, чтобы сравнение было регистронезависимым.

#LAB1\TASK2
Объяснение работы программы
Функция get_html_from_url :
Отправляет HTTP-запрос на указанный URL с помощью библиотеки requests.
Проверяет статус ответа (response.raise_for_status()), чтобы убедиться, что запрос успешен.
Устанавливает кодировку ответа как UTF-8.
Возвращает HTML-код страницы или None, если произошла ошибка.
Функция find_next_hyperlink :
Ищет ближайший тег <a> с атрибутом href, начиная с указанной позиции.
Возвращает кортеж (end_pos, href, name) или None, если тег не найден.
Функция find_all_hyperlinks :
Использует функцию find_next_hyperlink в цикле для поиска всех гиперссылок в документе.
Начинает поиск с позиции current_pos = 0 и обновляет её после каждого найденного тега.
Возвращает список кортежей (href, name).
Основная программа :
Задаёт URL страницы.
Получает HTML-код страницы с помощью get_html_from_url.
Ищет все гиперссылки с помощью find_all_hyperlinks.
Выводит найденные ссылки с порядковыми номерами.

Как программа получает HTML-документ?
Программа отправляет HTTP-запрос к указанному URL с помощью библиотеки requests и получает HTML-код страницы.
Что происходит, если страница недоступна?
Если запрос завершается ошибкой (например, 404 или отсутствие соединения), программа выводит сообщение об ошибке и завершает работу.
Как работает поиск всех ссылок?
Программа использует функцию find_next_hyperlink в цикле, начиная с позиции 0 и обновляя её после каждого найденного тега.
Почему используется регулярное выражение?
Регулярное выражение позволяет эффективно находить теги <a> с атрибутом href и извлекать их содержимое.
Как обрабатываются пробелы в тексте ссылки?
Метод .strip() удаляет лишние пробелы в начале и конце текста ссылки.
Можно ли использовать BeautifulSoup вместо регулярных выражений?
Да, можно. Однако регулярные выражения в данном случае проще и быстрее для задачи поиска конкретных тегов.
Метод endswith('.pdf') проверяет, заканчивается ли строка на .pdf.

9. Зачем используется urljoin(url, href)?
Многие ссылки на странице могут быть относительными (например, docs/example.pdf), а не абсолютными (например, https://pcoding.ru/docs/example.pdf).
Метод urljoin из модуля urllib.parse преобразует относительную ссылку в абсолютную, используя базовый URL (https://pcoding.ru/darkNet.php).

LAB1\task3
Как это работает:
Запрос к серверу :
Функция отправляет HTTP-запрос на указанный URL с помощью метода requests.get(url).
Сервер возвращает HTML-код страницы.
Проверка статуса ответа :
Метод response.raise_for_status() проверяет, успешен ли запрос (например, код 200 — успех, 404 — страница не найдена).
Если произошла ошибка (например, нет соединения), выбрасывается исключение, которое обрабатывается в блоке except.
Установка кодировки :
Сервер может использовать разные кодировки (например, UTF-8, Windows-1251). Мы явно указываем кодировку utf-8, чтобы корректно читать текст.
Возврат HTML-кода :
Если всё успешно, функция возвращает HTML-код страницы.
Если произошла ошибка, возвращается None.

Создание объекта BeautifulSoup :
HTML-код передаётся в конструктор BeautifulSoup, который преобразует его в дерево тегов для удобного поиска элементов.
Поиск блоков новостей :
Метод soup.find_all('div', class_='list-entry') находит все блоки с классом list-entry. Это основные контейнеры для новостей.
Мы используем срез [:10], чтобы взять только первые 10 новостей.
Извлечение данных из каждого блока :
Заголовок и ссылка :
Внутри блока ищется тег <h5>, а затем внутри него тег <a>.
Текст тега <a> становится заголовком новости, а значение атрибута href — ссылкой.
Дата :
Ищется тег <p>, а затем внутри него тег <small>. Текст этого тега становится датой.
Intro текст :
Ищется тег <div> с атрибутом style="text-align: justify;". Текст этого тега становится кратким описанием новости.
Добавление новости в список :
Каждая новость добавляется в список news_list в виде словаря с ключами "data", "href", "title", "text".
Возврат списка новостей :
После обработки всех блоков функция возвращает список новостей.

Как это работает:
Открытие файла :
Файл открывается в режиме записи ('w') с кодировкой utf-8.
Запись данных в JSON :
Метод json.dump преобразует список новостей в формат JSON и записывает его в файл.
Параметр ensure_ascii=False позволяет сохранять русские символы в читаемом виде.
Параметр indent=4 делает JSON-файл более читабельным за счёт отступов.
Подтверждение сохранения :
Выводится сообщение об успешном сохранении файла.

Задание URL :
Указывается URL страницы с новостями.
Получение HTML-кода :
Вызывается функция get_html_from_url, которая возвращает HTML-код страницы.
Парсинг новостей :
Если HTML-код успешно получен, вызывается функция parse_news, которая извлекает данные о новостях.
Сохранение в JSON :
Список новостей передаётся в функцию save_to_json, которая сохраняет их в файл news.json.
